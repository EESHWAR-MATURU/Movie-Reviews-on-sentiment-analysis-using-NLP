{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB-Dataset.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of numerical features : \n",
      "                                                    review sentiment\n",
      "count                                                4000      4000\n",
      "unique                                               3998         2\n",
      "top     My favourite police series of all time turns t...  negative\n",
      "freq                                                    2      2027\n",
      "\n",
      "Total number of reviews:  4000\n",
      "\n",
      "Total number of Sentiments:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Starting off, here's a synopsis: Porno queen A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>He pulled the guys guts out his butt! That's a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>This movie promised bat people. It didn't deli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>I saw the film many times, and every time I am...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>I loved KOLCHAK: THE NIGHT STALKER since I saw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  sentiment\n",
       "0     One of the other reviewers has mentioned that ...          1\n",
       "1     A wonderful little production. <br /><br />The...          1\n",
       "2     I thought this was a wonderful way to spend ti...          1\n",
       "3     Basically there's a family where a little boy ...          0\n",
       "4     Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "...                                                 ...        ...\n",
       "3995  Starting off, here's a synopsis: Porno queen A...          1\n",
       "3996  He pulled the guys guts out his butt! That's a...          0\n",
       "3997  This movie promised bat people. It didn't deli...          0\n",
       "3998  I saw the film many times, and every time I am...          0\n",
       "3999  I loved KOLCHAK: THE NIGHT STALKER since I saw...          1\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Summary statistics of numerical features : \\n\", df.describe())\n",
    "print(\"\\nTotal number of reviews: \",len(df))\n",
    "print(\"\\nTotal number of Sentiments: \", len(list(set(df['sentiment']))))\n",
    "df['sentiment'] = np.where(df['sentiment'] == \"positive\", 1, 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAFJCAYAAADe0EiPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAklUlEQVR4nO3de5xdVX338c/XRJByESgpxSQIarxEalFHxHpDsQo8CmotwlMFLDW1gq2FpxWrjyjWarXaSkU0VspFBfFCjYpSRJQ+rVwGQeQiOkaQRIQICmgQDP6eP/YeOQwzyUmYc3Zm8nm/XvOac9Zee+1fkkP4Zs3aa6eqkCRJkjRcD+q6AEmSJGlTZBCXJEmSOmAQlyRJkjpgEJckSZI6YBCXJEmSOmAQlyRJkjpgEJek9ZTkQ0n+7zSNtXOSnyeZ077/WpI/m46x2/G+lOTQ6RpvPa7790l+kuTHQ7rez5M8YhjXkqTpYhCXpB5JrktyZ5I7kvwsyf8keU2S3/x9WVWvqaq39znW89bWp6p+WFVbVdU901D7W5N8bML4+1bVKQ907PWsY2fgaGBxVf3uJMf3SvLrNjzfkeTaJK9aj/Hv94+V9vdw+QOvXpKGxyAuSff3oqraGng48C7gDcBHp/siSeZO95gbiZ2BW6rq5rX0+VFVbQVsA/w18JEkjxlKdZK0kTCIS9IUquq2qloGvBw4NMluAElOTvL37esdknyhnT2/Ncl/JXlQktNoAunn25nfv02yS5JKcniSHwJf7WnrDeWPTHJxktuTfC7J9u219kqyorfG8Vn3JPsAfwe8vL3et9rjv5k9but6c5Lrk9yc5NQkD22PjddxaJIftstK3jTV702Sh7bnr2rHe3M7/vOAc4GHtXWcvI7f46qqs4FbgSe0Y2/X/p6uSvLT9vWC9tg7gGcCH2jH/0DbXkke1fPnc0KSL7Yz7hcleWRP7c9vZ+FvS/LBJF+fzuVAktQvg7gkrUNVXQysoAmAEx3dHpsH7EgThquqXgn8kGZ2fauqenfPOc8GHge8YIpLHgL8KbATsAY4vo8avwz8A/DJ9nq/P0m3w9qv5wCPALYCPjChzzOAxwB7A29J8rgpLvmvwEPbcZ7d1vyqqvoKsC/tjHdVHba2utvwvj+wAzDWNj8I+Hean0jsDNw5XmdVvQn4L+DIdvwjpxj6IOBtwHbtuO9or7cD8GngjcBvA9cCf7C2GiVpUAziktSfHwHbT9L+K5rA/PCq+lVV/VdV1TrGemtV/aKq7pzi+GlVdWVV/QL4v8CB4zdzPkB/AryvqpZX1c9pwuhBE2bj31ZVd1bVt4BvAfcL9G0tBwFvrKo7quo64L3AK9ejlocl+RlNyD4LOKqqLgOoqluq6jNVtbqq7qAJ0c9ez1/rWVV1cVWtAT4O7N627wdcVVWfbY8dDwzlhlJJmsggLkn9mU+zfGKi99DMuP5nkuVJjuljrBvW4/j1wINpZowfqIe14/WOPZdmJn9cbyhdTTNrPtEObU0Tx5q/HrX8qKq2pVkjfjzw3PEDSX4ryYfbJS+3AxcA267nP0am+nU8jJ7f3/YfTfdZ7iNJw2IQl6R1SPIUmpD5/yYea2eEj66qRwD7A0cl2Xv88BRDrmvGfGHP651pZt1/AvwC+K2euubQLInpd9wf0Sz36B17DXDTOs6b6CdtTRPHWrme41BVd9HcDPt7SV7cNh9NszzmqVW1DfCstj3jp63vdXrcCCwYf5Mkve8laZgM4pI0hSTbJHkhcAbwsar69iR9XpjkUW2guw24B/h1e/gmmjXU6+sVSRYn+S3gOODT7faG3wUekuR/JXkw8GZg857zbgJ2Sc9WixOcDvx1kl2TbMW9a8rXrE9xbS1nAu9IsnWShwNHAR9b+5lTjnc3zdKWt7RNW9MsWflZe6PqsRNO2dDfV4Av0ob+dknOEcD9tliUpGEwiEvS/X0+yR00SxjeBLwPmGqf60XAV4CfA98APlhV57fH3gm8ud1R5f+sx/VPA06mWV7xEOAvodnFBXgt8G80s8+/4L7LKj7Vfr8lyTcnGfekduwLgB8AvwRetx519Xpde/3lND8p+EQ7/oY6Cdg5yYuAfwG2oJl5vxD48oS+7wde1u6oss4bWXtV1U+APwbeDdwCLAZGgbseQO2StEGy7nuKJEmandqfHqwA/qTnH1CSNBTOiEuSNilJXpBk2ySb02w3GZqZd0kaqs6DeJKT2gdLXDnF8SQ5PslYkiuSPGnYNUqSZpWnAd+nWfryIuDFa9lKUpIGpvOlKUmeRbO28tSq2m2S4/vRrEXcD3gq8P6qeupwq5QkSZKmV+cz4lV1AZPvzTvuAJqQXlV1Ic1esjsNpzpJkiRpMDoP4n2Yz30fbrGC9XtohCRJkrTRmbvuLjNHkiXAEoAtt9zyyY997GM7rkiSJEmz2aWXXlpVtUGT2zMhiK/kvk+ZW8AUT2+rqqXAUoCRkZEaHR0dfHWSJEnaZCXZ4Ju9Z8LSlGXAIe3uKXsCt1XVjV0XJUmSJD0Qnc+IJzkd2AvYIckKmkcZPxigqj4EnE2zY8oYsJqpn24nSZIkzRidB/GqOngdxws4YkjlSJIkSUMxE5amSJIkSbOOQVySJEnqgEFckiRJ6oBBXJIkSeqAQVySJEnqgEFckiRJ6oBBXJIkSeqAQVySJEnqwMCCeJKFSc5PcnWSq5L8Vdu+fZJzk3yv/b5d254kxycZS3JFkif1jHVo2/97SQ4dVM2SJEnSsAxyRnwNcHRVLQb2BI5Ishg4BjivqhYB57XvAfYFFrVfS4AToQnuNI+9fyqwB3DseHiXJEmSZqqBBfGqurGqvtm+vgO4BpgPHACc0nY7BXhx+/oA4NRqXAhsm2Qn4AXAuVV1a1X9FDgX2GdQdUuSJEnDMJQ14kl2AZ4IXATsWFU3tod+DOzYvp4P3NBz2oq2bar2ya6zJMloktHpq16SJEmafgMP4km2Aj4DvL6qbu89VlUF1HRdq6qWVtVIVY1M15iSJEnSIAw0iCd5ME0I/3hVfbZtvqldckL7/ea2fSWwsOf0BW3bVO2SJEnSjDXIXVMCfBS4pqre13NoGTC+88mhwOd62g9pd0/ZE7itXcJyDvD8JNu1N2k+v22TJEmSZqy5Axz76cArgW8nubxt+zvgXcCZSQ4HrgcObI+dDewHjAGrgVcBVNWtSd4OXNL2O66qbh1g3ZIkSdLApVmmPfuMjIzU6Kj3bEqSJGlwkqyuqi035FyfrClJkiR1wCAuSZIkdcAgLkmSJHXAIC5JkiR1wCAuSZIkdcAgLkmSJHXAIC5JkiR1wCAuSZIkdcAgLkmSJHXAIC5JkiR1wCAuSZIkdcAgLkmSJHXAIC5JkiR1wCAuSZIkdcAgLkmSJHXAIC5JkiR1wCAuSZIkdcAgLkmSJHXAIC5JkiR1wCAuSZIkdcAgLkmSJHXAIC5JkiR1wCAuSZIkdWBgQTzJSUluTnJlT9snk1zefl2X5PK2fZckd/Yc+1DPOU9O8u0kY0mOT5JB1SxJkiQNy9wBjn0y8AHg1PGGqnr5+Osk7wVu6+n//arafZJxTgReDVwEnA3sA3xp+suVJEmShmdgM+JVdQFw62TH2lntA4HT1zZGkp2AbarqwqoqmlD/4mkuVZIkSRq6rtaIPxO4qaq+19O2a5LLknw9yTPbtvnAip4+K9q2SSVZkmQ0yej0lyxJkiRNn66C+MHcdzb8RmDnqnoicBTwiSTbrO+gVbW0qkaqamSa6pQkSZIGYpBrxCeVZC7wUuDJ421VdRdwV/v60iTfBx4NrAQW9Jy+oG2TJEmSZrQuZsSfB3ynqlYAJNmn3RFlLMkxSR4BLAKWV9WNwC+TXJrkMuDDGMQlSZI0Cwxy+8LTgW8Aj0myIsnh7aGDaJelJJkDnAC8F7gbOBb4AvCaqhq/0XM5sBOwDfB54E8HVbMkSZI0LANbmlJVB0/RfljP2z2Asao6ETgxyRvbPp/v6bMKeH9V/WOSpwG7DqhkSZIkaWi6frLmfOCGnveT7YryVuAVSVbQ7CP+uqkG6901ZdWqVdNdqyRJkjRtug7i/TgYOLmqFgD7AaclmbTu3l1T5s2bN9QiJUmSpPXRdRBfCSzseT/ZriiHA2cCVNU3gIcAOwylOkmSJGlAug7ilwCLkuyaZDOaGzmXTejzQ2BvgCSPownirjuRJEnSjNZpEK+qNcCRwDnANcCZVXVVkuOS7N92Oxp4dZJv0ey2clj7uHtJkiRpxspszbQjIyM1OuqT7iVJkjQ4SVZX1ZYbcm7XS1MkSZKkTZJBXJIkSeqAQVySJEnqgEFckiRJ6oBBXJIkSeqAQVySJEnqgEFckiRJ6oBBXJIkSeqAQVySJEnqgEFckiRJ6oBBXJIkSeqAQVySJEnqgEFckiRJ6oBBXJIkSeqAQVySJEnqgEFckiRJ6sDAgniSk5LcnOTKnra3JlmZ5PL2a7+eY29MMpbk2iQv6Gnfp20bS3LMoOqVJEmShmmQM+InA/tM0v7PVbV7+3U2QJLFwEHA49tzPphkTpI5wAnAvsBi4OC2ryRJkjSjzR3UwFV1QZJd+ux+AHBGVd0F/CDJGLBHe2ysqpYDJDmj7Xv1dNcrSZIkDVMXa8SPTHJFu3Rlu7ZtPnBDT58VbdtU7ZIkSdKMNuwgfiLwSGB34EbgvdM5eJIlSUaTjE7nuJIkSdJ0G2oQr6qbquqeqvo18BHuXX6yEljY03VB2zZV+1TjL62qkaoamd7KJUmSpOk11CCeZKeety8BxndUWQYclGTzJLsCi4CLgUuARUl2TbIZzQ2dy4ZZsyRJkjQIA7tZM8npwF7ADklWAMcCeyXZHSjgOuDPAarqqiRn0tyEuQY4oqruacc5EjgHmAOcVFVXDapmSZIkaVhSVV3XMBAjIyM1OupScUmSJA1OktVVteWGnOuTNSVJkqQOGMQlSZKkDhjEJUmSpA4YxCVJkqQOGMQlSZKkDhjEJUmSpA4YxCVJkqQOGMQlSZKkDhjEJUmSpA4YxCVJkqQOGMQlSZKkDhjEJUmSpA4YxCVJkqQOGMQlSZKkDhjEJUmSpA4YxCVJkqQOGMQlSZKkDhjEJUmSpA4YxCVJkqQOGMQlSZKkDnQexJPsk+TaJGNJjpmiz4FJrk5yVZJPDLtGSZIkabrN7fLiSeYAJwB/CKwALkmyrKqu7umzCHgj8PSq+mmS3+mmWkmSJGn6dD0jvgcwVlXLq+pu4AzggAl9Xg2cUFU/Baiqm4dcoyRJkjTtBhbEk5yU5OYkV/a0vSfJd5JckeQsYBFwQ5JdktwJvAY4KsmHeoZ6CvDOJHcmuTHJPmu55pIko0lGV61aNahfmiRJkvSADXJG/GRgYmg+F9itqp4AfBd4ac+x7wNvBj5ZVa/pad8duArYBrgWOC3JtpNdsKqWVtVIVY3MmzdvOn4NkiRJ0kAMLIhX1QXArRPa/rOq1rRvLwS2ABb2dFkArBx/k2Sn9uUpVfUr4MPA3TQz6ZIkSdKM1eUa8T8FPk4TqhcAuwJvAV6a5Jltn/nAdcBe7fs7gO2B5cMsVJIkSZpuneyakuRNwBrgNOAnwL8DNwP/BnwZ+GqSJTTLVW4BbklyNbA5cE1V3TLFuEuAJQBPfvKTB/3LkCRJkjbY0IN4ksOAFwJ7V1UBZ7dfvX0uownhK4EFVfU8mps4D+be2fH7qaqlwFKAkZGRGkT9kiRJ0nQY6tKUdseTvwX2r6rVPe3z2j3FSfIImuUqy6vqRuD2JHsmCXAI8Llh1ixJkiQNwsBmxJOcTjN7vUOSFcCxNA/m2Rw4t8nVXNjukPIs4LgkvwJ+DbymqsZv9HwtzQ4sWwBfar8kSZKkGS3N6pDZZ2RkpEZHR7suQ5IkSbNYktVVteWGnNv1kzUlSZKkTZJBXJIkSeqAQVySJEnqgEFckiRJ6oBBXJIkSeqAQVySJEnqQF9BPMnT+2mTJEmS1J9+Z8T/tc82SZIkSX1Y65M1kzwN+ANgXpKjeg5tA8wZZGGSJEnSbLauR9xvBmzV9tu6p/124GWDKkqSJEma7dYaxKvq68DXk5xcVdcPqSZJkiRp1lvXjPi4zZMsBXbpPaeqnjuIoiRJkqTZrt8g/ingQ8C/AfcMrhxJkiRp09BvEF9TVScOtBJJkiRpE9Lv9oWfT/LaJDsl2X78a6CVSZIkSbNYvzPih7bf/6anrYBHTG85kiRJ0qahryBeVbsOuhBJkiRpU9JXEE9yyGTtVXXq9JYjSZIkbRr6XZrylJ7XDwH2Br4JGMQlSZKkDdDv0pTX9b5Psi1wxiAKkiRJkjYF/e6aMtEvANeNS5IkSRuo3zXin6fZJQVgDvA44MxBFSVJkiTNdv2uEf+nntdrgOurasUA6pEkSZI2CX0tTamqrwPfAbYGtgPu7ue8JCcluTnJlT1t2yc5N8n32u/bte1JcnySsSRXJHlSzzmHtv2/l+TQya4lSZIkzSR9BfEkBwIXA38MHAhclORlfZx6MrDPhLZjgPOqahFwXvseYF9gUfu1BDixvfb2wLHAU4E9gGPHw7skSZI0U/W7NOVNwFOq6maAJPOArwCfXttJVXVBkl0mNB8A7NW+PgX4GvCGtv3UqirgwiTbJtmp7XtuVd3aXvtcmnB/ep+1S5IkSRudfndNedB4CG/dsh7nTrRjVd3Yvv4xsGP7ej5wQ0+/FW3bVO2SJEnSjNVvmP5yknOSHJbkMOCLwNkP9OLt7Hets2OfkixJMppkdLrGlCRJkgZhrUE8yaOSPL2q/gb4MPCE9usbwNINvOZN7ZIT2u/jM+0rgYU9/Ra0bVO1309VLa2qkaoa2cDaJEmSpKFY14z4vwC3A1TVZ6vqqKo6CjirPbYhlgHjO58cCnyup/2QdveUPYHb2iUs5wDPT7Jde5Pm89s2SZIkacZa182aO1bVtyc2VtW3J7kJ836SnE5zs+UOSVbQ7H7yLuDMJIcD19PswgLNUpf9gDFgNfCq9lq3Jnk7cEnb77jxGzclSZKkmWpdQXzbtRzbYl2DV9XBUxzae5K+BRwxxTgnASet63qSJEnSTLGupSmjSV49sTHJnwGXDqYkSZIkafZb14z464GzkvwJ9wbvEWAz4CUDrEuSJEma1dYaxKvqJuAPkjwH2K1t/mJVfXXglUmSJEmzWF9P1qyq84HzB1yLJEmStMnY0KdjSpIkSXoADOKSJElSBwzikiRJUgcM4pIkSVIHDOKSJElSBwzikiRJUgc6D+JJ9klybZKxJMespd8fJakkI8OsT5IkSRqEToN4kjnACcC+wGLg4CSLJ+m3NfBXwEXDrVCSJEkajK5nxPcAxqpqeVXdDZwBHDBJv7cD/wj8cpjFSZIkSYPSdRCfD9zQ835F2/YbSZ4ELKyqLw6zMEmSJGmQug7ia5XkQcD7gKP77L8kyWiS0VWrVg22OEmSJOkB6DqIrwQW9rxf0LaN2xrYDfhakuuAPYFlU92wWVVLq2qkqkbmzZs3oJIlSZKkB67rIH4JsCjJrkk2Aw4Clo0frKrbqmqHqtqlqnYBLgT2r6rRbsqVJEmSpkenQbyq1gBHAucA1wBnVtVVSY5Lsn+XtUmSJEmDlKrquoaBGBkZqdFRJ84lSZI0OElWV9WWG3Ju10tTJEmSpE2SQVySJEnqgEFckiRJ6oBBXJIkSerA0IN4ksckubzn6/Ykr0/y1iQre9r36znnjUnGklyb5AXDrlmSJEmabnOHfcGquhbYHSDJHJoH+JwFvAr456r6p97+SRbT7C/+eOBhwFeSPLqq7hlm3ZIkSdJ06nppyt7A96vq+rX0OQA4o6ruqqofAGPAHkOpTpIkSRqQroP4QcDpPe+PTHJFkpOSbNe2zQdu6Omzom2TJEmSZqzOgnj7SPv9gU+1TScCj6RZtnIj8N4NGHNJktEkPslHkiRJG7UuZ8T3Bb5ZVTcBVNVNVXVPVf0a+Aj3Lj9ZCSzsOW9B23Y/VbW0qkaqamSAdUuSJEkPWJdB/GB6lqUk2ann2EuAK9vXy4CDkmyeZFdgEXDx0KqUJEmSBmDou6YAJNkS+EPgz3ua351kd6CA68aPVdVVSc4ErgbWAEe4Y4okSZJmulRV1zUMxMjISI2OulRckiRJg5NkdVVtuSHndr1riiRJkrRJMohLkiRJHTCIS5IkSR0wiEuSJEkdMIhLkiRJHTCIS5IkSR0wiEuSJEkdMIhLkiRJHTCIS5IkSR0wiEuSJEkdMIhLkiRJHTCIS5IkSR0wiEuSJEkdMIhLkiRJHTCIS5IkSR0wiEuSJEkdMIhLkiRJHTCIS5IkSR0wiEuSJEkdMIhLkiRJHTCIS5IkSR0wiEuSJEkdMIhLkiRJHegsiCe5Lsm3k1yeZLRt2z7JuUm+137frm1PkuOTjCW5IsmTuqpbkiRJmg5dz4g/p6p2r6qR9v0xwHlVtQg4r30PsC+wqP1aApw49EolSZKkadR1EJ/oAOCU9vUpwIt72k+txoXAtkl26qA+SZIkaVp0GcQL+M8klyZZ0rbtWFU3tq9/DOzYvp4P3NBz7oq2TZIkSZqRugziz6iqJ9EsOzkiybN6D1ZV0YT1viVZkmR0fM25JEmStLHqLIhX1cr2+83AWcAewE3jS07a7ze33VcCC3tOX9C2TRxzaVWN9Kw5lyRJkjZKnQTxJFsm2Xr8NfB84EpgGXBo2+1Q4HPt62XAIe3uKXsCt/UsYZEkSZJmnLkdXXdH4Kwk4zV8oqq+nOQS4MwkhwPXAwe2/c8G9gPGgNXAq4ZfsiRJkjR9OgniVbUc+P1J2m8B9p6kvYAjhlCaJEmSNBQb2/aFkiRJ0iah8yCeZJ8k17ZPzTxmkuNHJbm6faLmeUke3kWdkiRJ0nTqNIgnmQOcQLOF4WLg4CSLJ3S7DBipqicAnwbePdwqJUmSpOnX9Yz4HsBYVS2vqruBM2ieovkbVXV+Va1u315Is3WhJEmSNKN1HcTX94mZhwNfmupg7wN9Vq1aNU0lSpIkSdOv6yDetySvAEaA90zVp/eBPvPmzRtecZIkSdJ66mof8XF9PTEzyfOANwHPrqq7hlSbJEmSNDBdz4hfAixKsmuSzYCDaJ6i+RtJngh8GNi/qm6eZAxJkiRpxuk0iFfVGuBI4BzgGuDMqroqyXFJ9m+7vQfYCvhUksuTLJtiOEmSJGnGSPPQytlnZGSkRkdHuy5DkiRJs1iS1VW15Yac2/XSFEmSJGmTZBCXJEmSOmAQlyRJkjpgEJckSZI6YBCXJEmSOmAQlyRJkjpgEJckSZI6YBCXJEmSOmAQlyRJkjpgEJckSZI6YBCXJEmSOmAQlyRJkjpgEJckSZI6MPQgnmRhkvOTXJ3kqiR/1ba/NcnKJJe3X/v1nPPGJGNJrk3ygmHXLEmSJE23uR1ccw1wdFV9M8nWwKVJzm2P/XNV/VNv5ySLgYOAxwMPA76S5NFVdc9Qq5YkSZKm0dBnxKvqxqr6Zvv6DuAaYP5aTjkAOKOq7qqqHwBjwB6Dr1SSJEkanE7XiCfZBXgicFHbdGSSK5KclGS7tm0+cEPPaSuYIrgnWZJkNMnooGqWJEmSpkNnQTzJVsBngNdX1e3AicAjgd2BG4H3ru+YVbW0qkaqamQ6a5UkSZKmWydBPMmDaUL4x6vqswBVdVNV3VNVvwY+wr3LT1YCC3tOX9C2SZIkSTNWF7umBPgocE1Vva+nfaeebi8BrmxfLwMOSrJ5kl2BRcDFw6pXkiRJGoQudk15OvBK4NtJLm/b/g44OMnuQAHXAX8OUFVXJTkTuJpmx5Uj3DFFkiRJM12qqusaBmJkZKRGR71nU5IkSYOTZHVVbbkh5/pkTUmSJKkDBnFJkiSpAwZxSZIkqQMGcUmSJKkDBnFJkiSpAwZxSZIkqQMGcUmSJKkDBnFJkiSpAwZxSZIkqQMGcUmSJKkDBnFJkiSpAwZxSZIkqQMGcUmSJKkDBnFJkiSpAwZxSZIkqQMGcUmSJKkDBnFJkiSpAwZxSZIkqQMGcUmSJKkDBnFJkiSpAwZxSZIkqQMGcUmSJKkDBnFJkiSpA50H8ST7JLk2yViSYyY5vnmSTyb5UZI7k1w3WT9JkiRpJuk0iCeZA5wA7AssBg5OsnhCt8OBnwF3Am8ALpminyRJkjRjdD0jvgcwVlXLq+pu4AzggAl9DgBGgTHgg8BzpugnSZIkzRhzO77+fOCGnvcrgKdO0ifADVW1JsltNDPkj584WJLTgJf2vF893QVrRpsLrOm6CDaeOtTwz0OT8XOx8dkY/kw2hhq08dliQ0/sOohPq6p6JfBKgCSjVTXScUnaiGwsn4mNpQ41/PPQZPxcbHw2hj+TjaEGbXySjG7ouV0vTVkJLOx5v6Btm9ingIVJ5gIPBbadpJ8kSZI0Y3QdxC8BFiXZNclmwEHAsgl9lgEjwCLgL4CvTdFPkiRJmjE6DeJVtQY4EjgHuAY4s6quSnJckv3bbh8FtgceArwbeMp4v3UMv3RAZWvm2lg+ExtLHWr456HJ+LnY+GwMfyYbQw3a+Gzw5yJVNZ2FSJIkSepD10tTJEmSpE2SQVySJEnqwIwO4kn2SXJtkrHJHnufZPMkn2yPX5Rklw7K1JD18bk4KsnVSa5Icl6Sh3dRp4ZrXZ+Lnn5/lKSSuEXZJqCfz0WSA9u/M65K8olh16jh6+P/IzsnOT/JZe3/S/brok4NT5KTktyc5MopjifJ8e1n5ookT+pr3Jm6RjzJHOC7wB/SPAjoEuDgqrq6p89rgSdU1WuSHAS8pKpe3knBGoo+PxfPAS6qqtVJ/gLYa5CfiySPpXkS7Py2aSWwrKquGdQ1dV/9fC7aflsDXwQ2A46sqg3eG1Ybvz7/vlgEnAk8t6p+muR3qurmTgrWUPT5uVgKXFZVJyZZDJxdVbt0Ua+GI8mzgJ8Dp1bVbpMc3w94HbAfzcMp319VEx9SeT8zeUZ8D2CsqpZX1d1M/tj7A4BT2tefBvZOkiHWqOFb5+eiqs6vqvGnrl5Is3/9QCR5Q1tDgIvbrwCnr21WVtOun78vAN4O/CPwy2EWp87087l4NXBCVf0UwBDevSSvGvAl+vlcFLBN+/qhwI8GXJM6VlUXALeupcsBNCG9qupCYNskO61r3JkcxOcDN/S8X8G9M47369NulXgb8NtDqU5d6edz0etw4EsDrOdw4ClV9a6q+lj79S6av+gPH+B1dV/r/Fy0P0ZcWFVfHGZh6lQ/f188Gnh0kv9OcmGSfYZWnabytgGP38/n4q3AK5KsAM6mmQnVpm198wcwyx5xL62PJK+geVjUswd4mV8DDwOun9C+U3tMG4EkDwLeBxzWcSna+MyleaDcXjQ/Pbsgye9V1c+6LGq2S3LFVIeAHYdZyxQOBk6uqvcmeRpwWpLdqsq/17VeZnIQXwks7Hm/gPs/9n68z4okc2l+fHTLcMpTR/r5XJDkecCbgGdX1V0DrOf1wHlJvse9/1LeGXgUzcOsNBzr+lxsDewGfK1dvfa7wLIk+7tOfFbr5++LFTT3lPwK+EGS79IE80uGU+Ima0fgBcBPJ7QH+J8BX7ufz8XhwD4AVfWNJA8BdgBcurTp6it/TDSTl6ZcAixKsmuSzZj8sffLgEPb1y8Dvloz9e5U9Wudn4skTwQ+DOw/6PWeVfVlmh9tv43mCbLn0PxI8zHtMQ3HWj8XVXVbVe1QVbu0N1xdSPP5MITPbv38f+Q/aGbDSbIDzX/Py4dY46bqC8BWVXX9hK/rgK8N+Nr9fC5+COwNkORxNE//XjXgurRxWwYc0u6esidwW1XduK6TZuyMeFWtSXIkTbCZA5xUVVclOQ4YraplwEdpflw0RrPA/qDuKtYw9Pm5eA+wFfCpdvbzh1W1/wBr+jVNsFNH+vxcaBPT5+fiHOD5Sa4G7gH+pqr8yeqAVdWU99BU1f8e8LX7+VwcDXwkyV/T3Lh5mBN9s1uS02n+Ub5De2/AscCDAarqQzT3CuwHjAGrgb5uKp6x2xdKkiRJM9lMXpoiSZIkzVgGcUmaRZLck+TyJFcm+XySbdfRf/fepwIm2d897iVpOFyaIkmzSJKfV9VW7etTgO9W1TvW0v8wYKSq3MVHkoZsxt6sKUlap28ATwBIsgfwfprdHe6kuZHoB8BxwBZJngG8E9iCNpgnORm4nWa//d8F/raqPt3uu/4B4Lk023L+iuaGtk8P8dcmSTOeS1MkaRZKModme7XxHWG+Azyzqp4IvAX4h/bx3W8BPllVu1fVJycZaifgGcALgXe1bS8FdgEWA68EnjaoX4ckzWbOiEvS7LJFkstpHq18DXBu2/5Q4JQki2i2W3twn+P9R7sF59VJxp9o+AzgU237j5OcP23VS9ImxBlxSZpd7qyq3YGH0zyF8Ii2/e3A+VW1G/AimiUq/eh98mymq0hJkkFckmalqloN/CVwdJK5NDPi449bPqyn6x3A1us5/H8Df5TkQe0s+V4PrFpJ2jQZxCVplqqqy4ArgIOBdwPvTHIZ912WeD6wuN3y8OV9Dv0ZYAVwNfAx4JvAbdNWuCRtIty+UJK03pJsVVU/T/LbwMXA06vqx13XJUkziTdrSpI2xBfahwVtBrzdEC5J688ZcUmSJKkDrhGXJEmSOmAQlyRJkjpgEJckSZI6YBCXJEmSOmAQlyRJkjpgEJckSZI68P8BVneqqqcREwAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "df['sentiment'].value_counts().sort_index().plot(kind='bar',color = 'blue')\n",
    "plt.title('Distribution of Rating')\n",
    "plt.grid()\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>When thinking of the revelation that the main ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>This must have been one of the worst movies I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>A group of tourists are stranded on Snake Isla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>Silly movie is really, really funny. Yes, it's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>After hearing about George Orwell's prophetic ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>Excellent episode movie ala Pulp Fiction. 7 da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>The Bone Snatcher is about a group miners who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>Well, to each his own, but I thought Gibson's ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>In my analysis of \"Trois couleurs: Blanc\" I wr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>Many times the description \"full of sound and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  sentiment\n",
       "2230  When thinking of the revelation that the main ...          0\n",
       "668   This must have been one of the worst movies I ...          0\n",
       "3616  A group of tourists are stranded on Snake Isla...          0\n",
       "2363  Silly movie is really, really funny. Yes, it's...          1\n",
       "142   After hearing about George Orwell's prophetic ...          0\n",
       "...                                                 ...        ...\n",
       "2895  Excellent episode movie ala Pulp Fiction. 7 da...          1\n",
       "2140  The Bone Snatcher is about a group miners who ...          0\n",
       "3599  Well, to each his own, but I thought Gibson's ...          0\n",
       "2567  In my analysis of \"Trois couleurs: Blanc\" I wr...          1\n",
       "2067  Many times the description \"full of sound and ...          0\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.1, random_state=0) \n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 360 training examples and 40 validation examples. \n",
      "\n",
      "Show a review in the training set : \n",
      " Made and released at the time when the internet was just becoming huge, this is a storyline Hitchcock would have loved.<br /><br />Sadly, Hitchcock wasn't around to make it, and we're left with an occasionally suspenseful but mostly silly thriller, that is held (barely) together by Bullock's intelligence.<br /><br />It was released in 1995 but is already dated, and the amount of mistakes and inaccuaracies regarding computers must be seen to be believed, and you don't even have to be a dot.com person to spot them!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2933    A Scanner Darkly, Minority Report, Blade Runne...\n",
       " 352     Tell the truth Iâ€™m a bit stun to see all these...\n",
       " 1791    **Might contain spoilers**<br /><br />Ok, lets...\n",
       " 668     This must have been one of the worst movies I ...\n",
       " 2278    I like science-fiction movies and even, low-ra...\n",
       "                               ...                        \n",
       " 1888    Absolutely nothing is redeeming about this tot...\n",
       " 2390    WWE has produced some of the worst pay-per-vie...\n",
       " 1069    My comment is mainly a comment on the first co...\n",
       " 1593    This is indeed the film that popularized kung ...\n",
       " 751     They must issue this plot outline to all wanna...\n",
       " Name: review, Length: 360, dtype: object,\n",
       " 2933    1\n",
       " 352     0\n",
       " 1791    0\n",
       " 668     0\n",
       " 2278    0\n",
       "        ..\n",
       " 1888    0\n",
       " 2390    1\n",
       " 1069    1\n",
       " 1593    0\n",
       " 751     0\n",
       " Name: sentiment, Length: 360, dtype: int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], \\\n",
    "                                                    test_size=0.1, random_state=0)\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
    "print('Show a review in the training set : \\n', X_train.iloc[10])\n",
    "X_train,y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words\n",
    "Step 1 : Preprocess raw reviews to cleaned reviews\n",
    "\n",
    "Step 2 : Create BoW using CountVectorizer / Tfidfvectorizer in sklearn\n",
    "\n",
    "Step 3 : Transform review text to numerical representations (feature vectors)\n",
    "\n",
    "Step 4 : Fit feature vectors to supervised learning algorithm (eg. Naive Bayes, Logistic regression, etc.)\n",
    "\n",
    "Step 5 : Improve the model performance by GridSearch\n",
    "\n",
    "Text Preprocessing\n",
    "Step 1 : remove html tags using BeautifulSoup\n",
    "\n",
    "Step 2 : remove non-character such as digits and symbols\n",
    "\n",
    "Step 3 : convert to lower case\n",
    "\n",
    "Step 4 : remove stop words such as \"the\" and \"and\" if needed\n",
    "\n",
    "Step 5 : convert to root words by stemming if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from bs4 import BeautifulSoup \n",
    "import logging\n",
    "from wordcloud import WordCloud\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'html.parser').get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = letters_only.lower().split() \n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True:\n",
    "\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:\n",
    "        return (words)\n",
    "    return( \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show a cleaned review in the training set : \n",
      " made and released at the time when the internet was just becoming huge this is a storyline hitchcock would have loved sadly hitchcock wasn t around to make it and we re left with an occasionally suspenseful but mostly silly thriller that is held barely together by bullock s intelligence it was released in but is already dated and the amount of mistakes and inaccuaracies regarding computers must be seen to be believed and you don t even have to be a dot com person to spot them\n"
     ]
    }
   ],
   "source": [
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "    \n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer with Mulinomial Naive Bayes (Benchmark Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10208 \n",
      "\n",
      "Show some feature names: \n",
      " ['aaargh' 'bodies' 'corny' 'engrained' 'hackett' 'klein' 'nearby' 'props'\n",
      " 'severely' 'teens' 'wildlife']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "countVect = CountVectorizer() \n",
    "X_train_countVect = countVect.fit_transform(X_train_cleaned)\n",
    "print(\"Number of features: %d \\n\" % len(countVect.get_feature_names_out()))\n",
    "print(\"Show some feature names: \\n\", countVect.get_feature_names_out()[::1000])\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(countVect,open('countVect_imdb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "def modelEvaluation(predictions):\n",
    "    '''\n",
    "    Print model evaluation to predicted result \n",
    "    '''\n",
    "    print(\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.7500\n",
      "\n",
      "AUC score : 0.7444\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.86      0.78        21\n",
      "           1       0.80      0.63      0.71        19\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.76      0.74      0.74        40\n",
      "weighted avg       0.76      0.75      0.75        40\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[18  3]\n",
      " [ 7 12]]\n"
     ]
    }
   ],
   "source": [
    "predictions = mnb.predict(countVect.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(mnb,open('Naive_Bayes_model_imdb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1666 \n",
      "\n",
      "Show some feature names: \n",
      " ['10' 'normal']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5) \n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "print(\"Number of features: %d \\n\" % len(feature_names))\n",
    "print(\"Show some feature names: \\n\", feature_names[::1000])\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features with smallest coefficients :\n",
      "['bad' 'worst' 'like' 'br' 'plot' 'no' 'even' 'awful' 'terrible' 'could']\n",
      "\n",
      "Top 10 features with largest coefficients : \n",
      "['and' 'great' 'love' 'is' 'film' 'it' 'of' 'wonderful' 'other' 'always']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8750\n",
      "\n",
      "AUC score : 0.8784\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87        21\n",
      "           1       0.82      0.95      0.88        19\n",
      "\n",
      "    accuracy                           0.88        40\n",
      "   macro avg       0.88      0.88      0.87        40\n",
      "weighted avg       0.88      0.88      0.87        40\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[17  4]\n",
      " [ 1 18]]\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best paramenter set is : \n",
      " {'lr__C': 10, 'tfidf__max_features': None, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
      "\n",
      "Accuracy on validation set: 0.8250\n",
      "\n",
      "AUC score : 0.8208\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        21\n",
      "           1       0.88      0.74      0.80        19\n",
      "\n",
      "    accuracy                           0.82        40\n",
      "   macro avg       0.83      0.82      0.82        40\n",
      "weighted avg       0.83      0.82      0.82        40\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[19  2]\n",
      " [ 5 14]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\n",
    "model = Pipeline(estimators)\n",
    "params = {\"lr__C\":[0.1, 1, 10], \n",
    "          \"tfidf__min_df\": [1, 3], \n",
    "          \"tfidf__max_features\": [1000, None], \n",
    "          \"tfidf__ngram_range\": [(1,1), (1,2)], \n",
    "          \"tfidf__stop_words\": [None, \"english\"]} \n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid.fit(X_train_cleaned, y_train)\n",
    "print(\"The best paramenter set is : \\n\", grid.best_params_)\n",
    "predictions = grid.predict(X_test_cleaned)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "Step 1 : Parse review text to sentences (Word2Vec model takes a list of sentences as inputs)\n",
    "\n",
    "Step 2 : Create volcabulary list using Word2Vec model.\n",
    "\n",
    "Step 3 : Transform each review into numerical representation by computing average feature vectors of words therein.\n",
    "\n",
    "Step 4 : Fit the average feature vectors to Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['made', 'and', 'released', 'at', 'the', 'time', 'when', 'the', 'internet', 'was', 'just', 'becoming', 'huge', 'this', 'is', 'a', 'storyline', 'hitchcock', 'would', 'have', 'loved', 'sadly', 'hitchcock', 'wasn', 't', 'around', 'to', 'make', 'it', 'and', 'we', 're', 'left', 'with', 'an', 'occasionally', 'suspenseful', 'but', 'mostly', 'silly', 'thriller', 'that', 'is', 'held', 'barely', 'together', 'by', 'bullock', 's', 'intelligence', 'it', 'was', 'released', 'in', 'but', 'is', 'already', 'dated', 'and', 'the', 'amount', 'of', 'mistakes', 'and', 'inaccuaracies', 'regarding', 'computers', 'must', 'be', 'seen', 'to', 'be', 'believed', 'and', 'you', 'don', 't', 'even', 'have', 'to', 'be', 'a', 'dot', 'com', 'person', 'to', 'spot', 'them']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "\n",
    "    if isinstance(review, str):\n",
    "        review = review.strip()\n",
    "    else:\n",
    "        review = ' '.join(map(str, review))\n",
    "\n",
    "    raw_sentences = tokenizer.tokenize(review)\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer, remove_stopwords=False)\n",
    "    \n",
    "print('%d parsed sentence in the training set\\n'  % len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n', sentences[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Volcabulary List usinhg Word2Vec Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n",
      "Number of words in the vocabulary list : 966 \n",
      "\n",
      "Show first 10 words in the vocabulary list: \n",
      " ['the', 'a', 'and', 'of', 'to', 'is', 'it', 'in', 'i', 'this']\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec  \n",
    "num_features = 300  \n",
    "min_word_count = 10\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count=min_word_count,\n",
    "               window=context, sample=downsampling)  \n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_300features_10minwordcounts_10context\") \n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" % len(w2v.wv.index_to_key)) \n",
    "print(\"Show first 10 words in the vocabulary list: \\n\", w2v.wv.index_to_key[:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(review, model, num_features):\n",
    "    '''\n",
    "    Transform a review to a feature vector by averaging feature vectors of words \n",
    "    appeared in that review and in the volcabulary list created\n",
    "    '''\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word) \n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "            isZeroVec = False\n",
    "    if isZeroVec == False:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    '''\n",
    "    Transform all reviews to feature vectors using makeFeatureVec()\n",
    "    '''\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 360 feature vectors with 300 dimensions\n",
      "Validation set: 40 feature vectors with 300 dimensions\n"
     ]
    }
   ],
   "source": [
    "def makeFeatureVec(review, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    if hasattr(model.wv, 'index2word'):\n",
    "        index2word_set = set(model.wv.index2word)\n",
    "    elif hasattr(model.wv, 'index_to_key'):\n",
    "        index2word_set = set(model.wv.index_to_key)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported Gensim version. Please upgrade to version 4.0.0 or later.\")\n",
    "    \n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model.wv.get_vector(word))\n",
    "            isZeroVec = False\n",
    "    \n",
    "    if isZeroVec:\n",
    "        return np.zeros((num_features,), dtype=\"float32\")\n",
    "    \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "    for i, review in enumerate(reviews):\n",
    "        reviewFeatureVecs[i] = makeFeatureVec(review, model, num_features)\n",
    "    return reviewFeatureVecs\n",
    "if hasattr(w2v.wv, 'index2word'):\n",
    "    index2word_set = set(w2v.wv.index2word)\n",
    "elif hasattr(w2v.wv, 'index_to_key'):\n",
    "    index2word_set = set(w2v.wv.index_to_key)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported Gensim version. Please upgrade to version 4.0.0 or later.\")\n",
    "\n",
    "X_train_cleaned_strings = [' '.join(map(str, item)) for item in X_train_cleaned]\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned_strings, w2v, num_features)\n",
    "print(\"Training set: %d feature vectors with %d dimensions\" % trainVector.shape)\n",
    "\n",
    "X_test_cleaned_strings = [' '.join(map(str, item)) for item in X_test_cleaned]\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned_strings, w2v, num_features)\n",
    "print(\"Validation set: %d feature vectors with %d dimensions\" % testVector.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.5500\n",
      "\n",
      "AUC score : 0.5489\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57        21\n",
      "           1       0.53      0.53      0.53        19\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.55      0.55      0.55        40\n",
      "weighted avg       0.55      0.55      0.55        40\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[12  9]\n",
      " [ 9 10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(trainVector, y_train)\n",
    "predictions = rf.predict(testVector)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM\n",
    "\n",
    "Step 1 : Prepare X_train and X_test to 2D tensor.\n",
    "\n",
    "Step 2 : Train a simple LSTM (embeddign layer => LSTM layer => dense layer).\n",
    "\n",
    "Step 3 : Compile and fit the model using log loss function and ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding, LSTM, SimpleRNN, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Embedding, LSTM, SimpleRNN, GRU, Conv1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (360, 200)\n",
      "X_test shape: (40, 200)\n",
      "y_train shape: (360, 4)\n",
      "y_test shape: (40, 4)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "top_words = 40000 \n",
    "maxlen = 200 \n",
    "batch_size = 62\n",
    "nb_classes = 4\n",
    "nb_epoch = 6\n",
    "\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "y_train_seq = to_categorical(y_train, nb_classes)\n",
    "y_test_seq = to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape)\n",
    "print('X_test shape:', X_test_seq.shape)\n",
    "print('y_train shape:', y_train_seq.shape)\n",
    "print('y_test shape:', y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, None, 128)         5120000   \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spati  (None, None, 128)         0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5252100 (20.04 MB)\n",
      "Trainable params: 5252100 (20.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation, SpatialDropout1D\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128))\n",
    "model1.add(SpatialDropout1D(0.2))\n",
    "model1.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6/6 [==============================] - 9s 599ms/step - loss: 0.6799 - accuracy: 0.4556\n",
      "Epoch 2/6\n",
      "6/6 [==============================] - 4s 616ms/step - loss: 0.4842 - accuracy: 0.5306\n",
      "Epoch 3/6\n",
      "6/6 [==============================] - 3s 572ms/step - loss: 0.3542 - accuracy: 0.5361\n",
      "Epoch 4/6\n",
      "6/6 [==============================] - 3s 529ms/step - loss: 0.3509 - accuracy: 0.4750\n",
      "Epoch 5/6\n",
      "6/6 [==============================] - 3s 577ms/step - loss: 0.3484 - accuracy: 0.5306\n",
      "Epoch 6/6\n",
      "6/6 [==============================] - 3s 523ms/step - loss: 0.3425 - accuracy: 0.6083\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.3478 - accuracy: 0.4750\n",
      "Test loss : 0.3478\n",
      "Test accuracy : 0.4750\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
    "\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 360)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_seq),len(y_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of weight matrix in the embedding layer :  (40000, 128)\n",
      "No weights in the hidden layer\n",
      "Size of weight matrix in the output layer :  (128, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of weight matrix in the embedding layer : \", \\\n",
    "      model1.layers[0].get_weights()[0].shape)\n",
    "if len(model1.layers) > 1:\n",
    "    hidden_layer_weights = model1.layers[1].get_weights()\n",
    "    if hidden_layer_weights:\n",
    "        print(\"Size of weight matrix in the hidden layer : \", \\\n",
    "              hidden_layer_weights[0].shape)\n",
    "    else:\n",
    "        print(\"No weights in the hidden layer\")\n",
    "if len(model1.layers) > 2:\n",
    "    output_layer_weights = model1.layers[2].get_weights()\n",
    "    if output_layer_weights:\n",
    "        print(\"Size of weight matrix in the output layer : \", \\\n",
    "              output_layer_weights[0].shape)\n",
    "    else:\n",
    "        print(\"No weights in the output layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model1,open('model1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix :  (966, 300)\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\n",
    "\n",
    "if hasattr(w2v.wv, 'vectors'):\n",
    "    embedding_matrix = w2v.wv.vectors\n",
    "    print(\"Shape of embedding matrix : \", embedding_matrix.shape)\n",
    "else:\n",
    "    print(\"The 'vectors' attribute is not available in the Word2Vec model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (360, 300)\n",
      "X_test shape: (40, 300)\n",
      "y_train shape: (360, 4)\n",
      "y_test shape: (40, 4)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "top_words = embedding_matrix.shape[0] \n",
    "maxlen = 300 \n",
    "batch_size = 62\n",
    "nb_classes = 4\n",
    "nb_epoch = 7\n",
    "\n",
    "tokenizer = Tokenizer(num_words=top_words) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq1 = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq1 = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "y_train_seq1 = to_categorical(y_train, nb_classes)\n",
    "y_test_seq1 = to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq1.shape)\n",
    "print('X_test shape:', X_test_seq1.shape)\n",
    "print('y_train shape:', y_train_seq1.shape)\n",
    "print('y_test shape:', y_test_seq1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 300)         289800    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               219648    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 509964 (1.95 MB)\n",
      "Trainable params: 509964 (1.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0],  \n",
    "                            embedding_matrix.shape[1],  \n",
    "                            weights=[embedding_matrix])\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  \n",
    "model2.add(Dense(nb_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
